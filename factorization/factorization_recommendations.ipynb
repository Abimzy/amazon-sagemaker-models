{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System with Amazon SageMaker Factorization Machines\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "- Recommender systems were a catalyst for ML's popularity (Amazon, Netflix Prize)\n",
    "- User item matrix factorization is a core methodology\n",
    "- Factorization machines combine linear prediction with a factorized representation of pairwise feature interaction\n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "- SageMaker has a highly scalable factorization machines algorithm built-in\n",
    "- To learn more about the math behind _factorization machines_, [this paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) is a great resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "base = 'DEMO-loft-recommender'\n",
    "prefix = 'sagemaker/' + base\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "[Amazon Reviews AWS Public Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "- 1 to 5 star ratings\n",
    "- 2M+ Amazon customers\n",
    "- 160K+ digital videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/recsys/’: File exists\n",
      "download: s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz to ../../../../../tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "Drop some fields that won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'product_title', 'star_rating', 'review_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users don't rate most movies - Check our long tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       4.0\n",
      "0.95       5.0\n",
      "0.96       6.0\n",
      "0.97       7.0\n",
      "0.98       9.0\n",
      "0.99      13.0\n",
      "1.00    2704.0\n",
      "Name: customer_id, dtype: float64\n",
      "products\n",
      " 0.00        1.00\n",
      "0.01        1.00\n",
      "0.02        1.00\n",
      "0.03        1.00\n",
      "0.04        1.00\n",
      "0.05        1.00\n",
      "0.10        1.00\n",
      "0.25        1.00\n",
      "0.50        3.00\n",
      "0.75        9.00\n",
      "0.90       31.00\n",
      "0.95       73.00\n",
      "0.96       95.00\n",
      "0.97      130.00\n",
      "0.98      199.00\n",
      "0.99      386.67\n",
      "1.00    32790.00\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out customers who haven't rated many movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sequential index for customers and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>10463</td>\n",
       "      <td>140451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>489</td>\n",
       "      <td>140451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44025160</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>32100</td>\n",
       "      <td>140451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18602179</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-12-23</td>\n",
       "      <td>2237</td>\n",
       "      <td>140451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14424972</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>32340</td>\n",
       "      <td>140451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id                          product_title  \\\n",
       "0     27072354  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "1     16030865  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "2     44025160  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "3     18602179  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "4     14424972  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "\n",
       "   star_rating review_date   user    item  \n",
       "0            5  2015-08-31  10463  140451  \n",
       "1            5  2014-06-20    489  140451  \n",
       "2            5  2014-05-27  32100  140451  \n",
       "3            5  2014-12-23   2237  140451  \n",
       "4            5  2015-08-31  32340  140451  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0]) + customer_index.shape[0]})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count days since first review (included as a feature to capture trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['review_date'] = pd.to_datetime(reduced_df['review_date'])\n",
    "customer_first_date = reduced_df.groupby('customer_id')['review_date'].min().reset_index()\n",
    "customer_first_date.columns = ['customer_id', 'first_review_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.merge(customer_first_date)\n",
    "reduced_df['days_since_first'] = (reduced_df['review_date'] - reduced_df['first_review_date']).dt.days\n",
    "reduced_df['days_since_first'] = reduced_df['days_since_first'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Factorization machines expects data to look something like:\n",
    "  - Sparse matrix\n",
    "  - Target variable is that user's rating for a movie\n",
    "  - One-hot encoding for users ($N$ features)\n",
    "  - One-hot encoding for movies ($M$ features)\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|Feature1|Feature2|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|20|2.2|...|\n",
    "|5|1|0|...|0|0|1|0|...|0|17|9.1|...|\n",
    "|3|0|1|...|0|1|0|0|...|0|3|11.0|...|\n",
    "|4|0|1|...|0|0|0|1|...|0|15|6.4|...|\n",
    "\n",
    "\n",
    "- Wouldn't want to hold this full matrix in memory\n",
    "  - Create a sparse matrix\n",
    "  - Designed to work efficiently with CPUs. Some parts of training for more dense matrices can be parallelized with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csr_matrix(df, num_users, num_items):\n",
    "    feature_dim = num_users + num_items + 1\n",
    "    data = np.concatenate([np.array([1] * df.shape[0]),\n",
    "                           np.array([1] * df.shape[0]),\n",
    "                           df['days_since_first'].values])\n",
    "    row = np.concatenate([np.arange(df.shape[0])] * 3)\n",
    "    col = np.concatenate([df['user'].values,\n",
    "                          df['item'].values,\n",
    "                          np.array([feature_dim - 1] * df.shape[0])])\n",
    "    return csr_matrix((data, (row, col)), \n",
    "                      shape=(df.shape[0], feature_dim), \n",
    "                      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = to_csr_matrix(train_df, customer_index.shape[0], product_index.shape[0])\n",
    "test_csr = to_csr_matrix(test_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to sparse recordIO-wrapped protobuf that SageMaker factorization machines expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3_protobuf(csr, label, bucket, prefix, channel='train', splits=10):\n",
    "    indices = np.array_split(np.arange(csr.shape[0]), splits)\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, csr[index, ], label[index])\n",
    "        buf.seek(0)\n",
    "        boto3.client('s3').upload_fileobj(buf, bucket, '{}/{}/data-{}'.format(prefix, channel, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(train_csr, train_df['star_rating'].values.astype(np.float32), bucket, prefix)\n",
    "to_s3_protobuf(test_csr, test_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='test', splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "- Create a [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) estimator to run a training jobs and specify:\n",
    "  - Algorithm container image\n",
    "  - IAM role\n",
    "  - Hardware setup\n",
    "  - S3 output location\n",
    "  - Algorithm hyperparameters\n",
    "    - `feature_dim`: $N + M + 1$ (additional feature is `days_since_first` to capture trend)\n",
    "    - `num_factors`: number of factor dimensions (increasing too much can lead to overfitting)\n",
    "    - `epochs`: number of full passes through the dataset\n",
    "- `.fit()` points to training and test data in S3 and begins the training job\n",
    "\n",
    "**Note**: For AWS accounts registered in conjunction with a workshop, default instance limits may prevent the use of `ml.c5.2xlarge` (and other equally powerful instances), and may require a lower value for `train_instance_count` depending on the instance type chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 05:40:47 Starting - Starting the training job...\n",
      "2020-01-09 05:40:49 Starting - Launching requested ML instances......\n",
      "2020-01-09 05:41:54 Starting - Preparing the instances for training......\n",
      "2020-01-09 05:43:11 Downloading - Downloading input data...\n",
      "2020-01-09 05:43:41 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'3', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'3', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 WARNING 140425530926912] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:43:56.262] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:43:56.269] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 71592}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] nvidia-smi took: 0.0251491069794 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:56 INFO 140425530926912] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 37.0631217956543, \"sum\": 37.0631217956543, \"min\": 37.0631217956543}}, \"EndTime\": 1578548636.304996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548636.25812}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1578548636.305182, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548636.305133}\n",
      "\u001b[0m\n",
      "\u001b[34m[05:43:56] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202222.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[05:43:56] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202222.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:58 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=4.48608395137\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:58 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=20.1249492188\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:43:58 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=4.37487792969\u001b[0m\n",
      "\n",
      "2020-01-09 05:43:54 Training - Training image download completed. Training in progress.\u001b[34m[01/09/2020 05:44:04 INFO 140425530926912] Iter[0] Batch [500]#011Speed: 74925.75 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:04 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=1.3681938372\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:04 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=1.87195437616\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:04 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=1.04648447246\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] Iter[0] Batch [1000]#011Speed: 79019.28 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=1000 train rmse <loss>=1.30283133455\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=1000 train mse <loss>=1.69736948629\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, batch=1000 train absolute_loss <loss>=1.01660035997\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:44:11.585] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 14102, \"num_examples\": 1029, \"num_bytes\": 73144820}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.30339351763\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.69883466181\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.01713547847\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"update.time\": {\"count\": 1, \"max\": 15281.06689453125, \"sum\": 15281.06689453125, \"min\": 15281.06689453125}}, \"EndTime\": 1578548651.586442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548636.305074}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1030, \"sum\": 1030.0, \"min\": 1030}, \"Total Records Seen\": {\"count\": 1, \"max\": 1029606, \"sum\": 1029606.0, \"min\": 1029606}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1578548651.586644, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1578548636.305338}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #throughput_metric: host=algo-1, train throughput=67310.9438074 records/second\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.04935980763\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.10115600586\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:11 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.87276739502\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/09/2020 05:44:17 INFO 140425530926912] Iter[1] Batch [500]#011Speed: 80728.14 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:17 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=1.1467941324\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:17 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=1.3151367821\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:17 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.91350606246\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] Iter[1] Batch [1000]#011Speed: 78669.17 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=1000 train rmse <loss>=1.19190317002\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=1000 train mse <loss>=1.4206331667\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, batch=1000 train absolute_loss <loss>=0.951340445992\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:44:24.518] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 12927, \"num_examples\": 1029, \"num_bytes\": 73144820}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1956909498\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.42967684744\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.953740091011\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12929.651975631714, \"sum\": 12929.651975631714, \"min\": 12929.651975631714}}, \"EndTime\": 1578548664.519361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548651.58651}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2059, \"sum\": 2059.0, \"min\": 2059}, \"Total Records Seen\": {\"count\": 1, \"max\": 2058212, \"sum\": 2058212.0, \"min\": 2058212}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1578548664.519604, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1578548651.589674}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #throughput_metric: host=algo-1, train throughput=79551.5995674 records/second\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.07539952151\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.15648413086\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:24 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.900743591309\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:31 INFO 140425530926912] Iter[2] Batch [500]#011Speed: 76926.17 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:31 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=1.14804383235\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:31 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=1.31800464099\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:31 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.915209358383\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] Iter[2] Batch [1000]#011Speed: 76922.64 samples/sec\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=1000 train rmse <loss>=1.19309492192\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=1000 train mse <loss>=1.42347549271\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, batch=1000 train absolute_loss <loss>=0.953379374654\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:44:37.909] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 13385, \"num_examples\": 1029, \"num_bytes\": 73144820}\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.19679964782\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.43232939701\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.955688844731\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, train rmse <loss>=1.19679964782\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, train mse <loss>=1.43232939701\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #quality_metric: host=algo-1, train absolute_loss <loss>=0.955688844731\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 13387.56799697876, \"sum\": 13387.56799697876, \"min\": 13387.56799697876}}, \"EndTime\": 1578548677.910296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548664.519438}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3088, \"sum\": 3088.0, \"min\": 3088}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086818, \"sum\": 3086818.0, \"min\": 3086818}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1578548677.910487, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1578548664.522696}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] #throughput_metric: host=algo-1, train throughput=76831.0936536 records/second\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 WARNING 140425530926912] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:37 INFO 140425530926912] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 21.703004837036133, \"sum\": 21.703004837036133, \"min\": 21.703004837036133}}, \"EndTime\": 1578548677.932446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548677.910358}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:38 INFO 140425530926912] Saved checkpoint to \"/tmp/tmpiUnhF4/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:44:38.931] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 42668, \"num_examples\": 1, \"num_bytes\": 70668}\u001b[0m\n",
      "\u001b[34m[2020-01-09 05:44:59.188] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 20257, \"num_examples\": 141, \"num_bytes\": 9937804}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1578548699.188907, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548678.930687}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #test_score (algo-1) : ('rmse', 65.1281040242052)\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #test_score (algo-1) : ('mse', 4241.6699337876935)\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #test_score (algo-1) : ('absolute_loss', 40.49613600109374)\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #quality_metric: host=algo-1, test rmse <loss>=65.1281040242\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #quality_metric: host=algo-1, test mse <loss>=4241.66993379\u001b[0m\n",
      "\u001b[34m[01/09/2020 05:44:59 INFO 140425530926912] #quality_metric: host=algo-1, test absolute_loss <loss>=40.4961360011\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 62968.29295158386, \"sum\": 62968.29295158386, \"min\": 62968.29295158386}, \"setuptime\": {\"count\": 1, \"max\": 32.87100791931152, \"sum\": 32.87100791931152, \"min\": 32.87100791931152}}, \"EndTime\": 1578548699.190095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1578548677.932515}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-01-09 05:45:00 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'factorization-machines', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=1, # Note: instance numbers may be limited on workshop credits \n",
    "    train_instance_type='ml.c5.xlarge', # Note:'ml.c5.2xlarge' may not be available on workshop credits\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    base_job_name=base,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=customer_index.shape[0] + product_index.shape[0] + 1,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=256,\n",
    "    epochs=3)\n",
    "\n",
    "fm.fit({'train': sagemaker.s3_input('s3://{}/{}/train/'.format(bucket, prefix), distribution='ShardedByS3Key'), \n",
    "        'test': sagemaker.s3_input('s3://{}/{}/test/'.format(bucket, prefix), distribution='FullyReplicated')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Deploy trained model to a real-time production endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup predictor to serialize in-memory data for invocation requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(df):\n",
    "    feature_dim = customer_index.shape[0] + product_index.shape[0] + 1\n",
    "    js = {'instances': []}\n",
    "    for index, data in df.iterrows():\n",
    "        js['instances'].append({'data': {'features': {'values': [1, 1, data['days_since_first']],\n",
    "                                                      'keys': [data['user'], data['item'], feature_dim - 1],\n",
    "                                                      'shape': [feature_dim]}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-time prediction for what a single user would rate an item**\n",
    "\n",
    "1. Pick a customer-movie pair from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pull out a single customer-movie pair that we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customer = test_df.iloc[[20]]\n",
    "test_df.iloc[[20]] # peek at the data to confirm it's the one we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pass `test_customer` to predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor.predict(test_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's make a df for an arbitrary customer and movie pair and test it out!**\n",
    "\n",
    "Our `fm_serializer` requires 3 inputs to perform a prediction:\n",
    " - `user` id for a customer (type = num)\n",
    " - `item` id for a movie (type = num)\n",
    " - `days_since_first` review (type = double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_customer = test_customer # make a copy of the test_customer we pulled out before to modify\n",
    "desired_user_id = 65884 # person who rated Dexter with 5 stars\n",
    "desired_item_id = 140461 # Code for True Blood: Season 1\n",
    "desired_review_days = 28.0 # arbitrary number of days since first review\n",
    "\n",
    "#fake_customer_data = {'user' : desired_user_id, 'item' : desired_item_id, 'days_since_first' : desired_review_days}\n",
    "#fake_customer = pd.DataFrame(fake_customer_data, index=[0])\n",
    "fake_customer['user'] = desired_user_id\n",
    "fake_customer['item'] = desired_item_id\n",
    "fake_customer['days_since_first'] = desired_review_days\n",
    "\n",
    "# print the details for this fake customer\n",
    "fake_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.predict(fake_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step: Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
